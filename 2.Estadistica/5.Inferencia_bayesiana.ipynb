{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9862c94f",
   "metadata": {},
   "source": [
    "# Razonamiento Estadístico: Inferencia Bayesiana y Procesos Estocásticos\n",
    "\n",
    "Hasta ahora, hemos explorado los fundamentos de la estadística clásica, también conocida como estadística frecuentista. En este paradigma, los parámetros poblacionales (como la media μ) son considerados como valores fijos pero desconocidos, y los datos de la muestra se ven como realizaciones aleatorias de un proceso. Sin embargo, existe un segundo gran paradigma, la **inferencia bayesiana**, que ofrece un enfoque conceptualmente diferente y cada vez más popular en la ciencia de datos. En el enfoque bayesiano, los parámetros poblacionales no se consideran fijos, sino que se tratan como variables aleatorias con sus propias distribuciones de probabilidad. Esto permite incorporar conocimiento previo o creencias sobre los parámetros antes de ver los datos, y luego actualizar estas creencias de manera sistemática a medida que se obtienen nuevos datos.\n",
    "\n",
    "## Teorema de Bayes\n",
    "\n",
    "El corazón de la inferencia bayesiana es el **Teorema de Bayes**, una fórmula fundamental de la probabilidad que describe cómo calcular la probabilidad condicional de un evento. En el contexto de la inferencia, se expresa como:\n",
    "\n",
    "`P(Parámetro | Datos) = [P(Datos | Parámetro) * P(Parámetro)] / P(Datos)`\n",
    "\n",
    "Donde:\n",
    "*   `P(Parámetro | Datos)` es la **probabilidad posterior**. Es la distribución actualizada del parámetro después de haber observado los datos. Es lo que buscamos al final.\n",
    "*   `P(Datos | Parámetro)` es la **verosimilitud**. Es la probabilidad de observar los datos dados un valor específico del parámetro. Captura la información que los datos aportan.\n",
    "*   `P(Parámetro)` es la **probabilidad previa** (o *prior*). Es la distribución de probabilidad que representa nuestras creencias sobre el parámetro antes de ver los datos. Puede basarse en experiencias anteriores, estudios o conocimiento experto.\n",
    "*   `P(Datos)` es la **evidencia marginal** o probabilidad de los datos, que actúa como un factor de normalización para asegurar que la distribución posterior integre a 1.\n",
    "\n",
    "El poder del enfoque bayesiano radica en su flexibilidad. Por ejemplo, si estamos desarrollando un modelo para detectar spam y tenemos acceso a un estudio previo que nos da una idea de la tasa de spam típica, podemos usar esa información como nuestra distribución *prior*. Una vez que obtenemos un nuevo lote de correos electrónicos, actualizamos nuestra creencia para obtener una distribución *posterior* más precisa. Este enfoque iterativo es natural y refleja cómo aprendemos en la vida cotidiana. Es especialmente útil en situaciones con datos limitados, ya que el *prior* ayuda a regularizar el modelo y evitar sobreajustes. Librerías como PyMC3 en Python facilitan la implementación de modelos bayesianos complejos.\n",
    "\n",
    "## Procesos Estocásticos\n",
    "\n",
    "Mientras que la inferencia bayesiana se enfoca en actualizar la creencia sobre parámetros, otra rama de la estadística nos ayuda a modelar secuencias de eventos aleatorios: la teoría de **procesos estocásticos**. Un proceso estocástico es una colección de variables aleatorias ordenadas en el tiempo (o en el espacio), que describe la evolución de un sistema aleatorio. Dos conceptos clave en este campo son los modelos de **Markov** y la distribución de **Poisson**.\n",
    "\n",
    "Los **modelos de Markov** son procesos estocásticos que cumplen con la propiedad de Markov: el futuro estado del sistema depende solo del estado presente y no de la secuencia de eventos que lo precedió. Esta propiedad \"sin memoria\" simplifica enormemente el modelado. Un modelo de Markov se define por un conjunto de estados y las probabilidades de transición entre ellos. Por ejemplo, podríamos modelar el clima de una ciudad como un modelo de Markov con estados \"soleado\", \"nublado\" y \"lluvioso\". Conociendo la probabilidad de que un día soleado sea seguido por otro soleado, o por un día lluvioso, podríamos predecir la probabilidad de un patrón climático a largo plazo. Los **cadenas de Markov** son una aplicación directa de esto y se utilizan en diversos campos, desde la biología para modelar secuencias de ADN hasta en marketing para modelar la trayectoria del usuario en un sitio web.\n",
    "\n",
    "Como ya discutimos, la distribución de **Poisson** modela el número de eventos que ocurren en un intervalo de tiempo o espacio. El proceso de Poisson es el proceso estocástico subyacente que describe la ocurrencia de estos eventos. Una de sus características clave es que el tiempo entre eventos consecutivos sigue una distribución exponencial. Por lo tanto, si el número de clics en un anuncio web sigue una distribución de Poisson, el tiempo que transcurre entre cada clic seguirá una distribución exponencial.\n",
    "\n",
    "En resumen, al avanzar en el razonamiento estadístico, encontramos herramientas que van más allá de los datos estáticos y fijos. La inferencia bayesiana nos brinda un marco dinámico para actualizar nuestra comprensión a medida que se acumula nueva evidencia. Por otro lado, los procesos estocásticos, como los modelos de Markov y el proceso de Poisson, nos permiten modelar sistemas que cambian con el tiempo de una manera rigurosa y probabilística. Estos principios avanzados son los que impulsan aplicaciones innovadoras en aprendizaje automático, finanzas, genómica y ciencias sociales, demostrando que la estadística es un campo vivo y en constante evolución."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
