{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed6cb27e",
   "metadata": {},
   "source": [
    "# Fundamentos de la Incertidumbre: La Teor√≠a de la Probabilidad\n",
    "\n",
    "Para que la inferencia estad√≠stica funcione, es necesario contar con un lenguaje preciso para cuantificar la incertidumbre. Ese lenguaje es la **teor√≠a de la probabilidad**. La probabilidad es una rama de las matem√°ticas que asigna un n√∫mero entre 0 y 1 (o entre 0% y 100%) a la ocurrencia de un evento, donde 0 significa que es imposible y 1 significa que es seguro. En el contexto de la ciencia de datos, la probabilidad nos permite modelar y gestionar la aleatoriedad inherente a casi todos los fen√≥menos del mundo real. Desde la incertidumbre en las mediciones experimentales hasta la variabilidad en el comportamiento del consumidor, la probabilidad proporciona los instrumentos para expresar y analizar esta incertidumbre de manera sistem√°tica.\n",
    "\n",
    "Un concepto central en la probabilidad es el de **variable aleatoria**. Una variable aleatoria es una funci√≥n que asigna un resultado num√©rico a cada posible resultado de un experimento aleatorio. Hay dos tipos principales:\n",
    "*   **Variables Aleatorias Discretas:** Toman un n√∫mero contable de valores, t√≠picamente enteros. Ejemplos incluyen el n√∫mero de caras al lanzar una moneda varias veces o el n√∫mero de clientes que llegan a una tienda en una hora. La probabilidad de cada valor posible se describe mediante una **funci√≥n de masa de probabilidad (PMF)**.\n",
    "*   **Variables Aleatorias Continuas:** Pueden tomar cualquier valor num√©rico dentro de un intervalo o rango continuo. Ejemplos incluyen la altura de una persona, el tiempo que tarda en llegar un autob√∫s o la temperatura en un d√≠a determinado. La probabilidad de que una variable continua tome un valor espec√≠fico es cero; en su lugar, se habla de la probabilidad de que caiga dentro de un cierto rango. Esta probabilidad se describe mediante una **funci√≥n de densidad de probabilidad (PDF)**, donde el √°rea bajo la curva entre dos puntos representa la probabilidad de que la variable caiga en ese intervalo.\n",
    "\n",
    "A partir de las variables aleatorias, se definen las **distribuciones de probabilidad**, que describen c√≥mo se distribuyen las probabilidades de los posibles valores de una variable aleatoria. Como mencionamos anteriormente, algunas distribuciones son tan comunes que se han convertido en herramientas esenciales en la ciencia de datos. La elecci√≥n de una distribuci√≥n para modelar un fen√≥meno de datos no es arbitraria; se basa en la naturaleza del problema: el tipo de datos (discreto o continuo), la simetr√≠a esperada, los l√≠mites de los valores y la frecuencia de los valores extremos.\n",
    "\n",
    "Aqu√≠ hay una tabla que resume algunas de las distribuciones de probabilidad m√°s importantes y sus aplicaciones en la ciencia de datos:\n",
    "\n",
    "| Distribuci√≥n | Tipo | Par√°metros Clave | Aplicaciones Comunes |\n",
    "| :--- | :--- | :--- | :--- | \n",
    "| **Normal / Gaussiana** | Continua | Media (Œº), Desviaci√≥n Est√°ndar (œÉ) | Modelado de errores de medici√≥n, alturas, IQ, rendimientos de activos financieros, residuos de modelos lineales. |\n",
    "| **Binomial** | Discreta | N√∫mero de ensayos (n), Probabilidad de √©xito (p) | Resultados de experimentos con dos salidas (√©xito/fracaso), tasas de conversi√≥n, pruebas cl√≠nicas. |\n",
    "| **Poisson** | Discreta | Tasa media de eventos (Œª) | Modelado de conteos de eventos raros en un intervalo (tiempo, espacio), llamadas a un centro de atenci√≥n, fallos de hardware. |\n",
    "| **Uniforme** | Discreta o Continua | M√≠nimo (a), M√°ximo (b) | Generadores de n√∫meros aleatorios, escenarios donde todos los resultados son igualmente probables. |\n",
    "| **Exponencial** | Continua | Tasa (Œº) | Modelado del tiempo entre eventos en un proceso de Poisson (tiempo de espera, vida √∫til de componentes). |\n",
    "| **t de Student** | Continua | Grados de libertad (df) | Inferencia sobre medias cuando la desviaci√≥n est√°ndar de la poblaci√≥n es desconocida y los datos son peque√±os. |\n",
    "| **Chi-cuadrado** | Continua | Grados de libertad (df) | Pruebas de bondad de ajuste, pruebas de independencia en tablas de contingencia (prueba Chi-cuadrado). |\n",
    "\n",
    "En resumen, la probabilidad no es solo un tema te√≥rico; es una herramienta pr√°ctica para modelar el mundo tal como es: lleno de incertidumbre. Proporciona la base matem√°tica para todo lo que sigue: desde el Teorema Central del L√≠mite hasta la construcci√≥n de intervalos de confianza y las pruebas de hip√≥tesis. Para cualquier aspirante a cient√≠fico de datos, dominar la intuici√≥n detr√°s de estas distribuciones y saber cu√°ndo aplicarlas es un paso decisivo hacia un an√°lisis de datos m√°s profundo y riguroso.\n",
    "\n",
    "## Dominando las Distribuciones: Herramientas para Modelar Fen√≥menos Reales\n",
    "\n",
    "Si la probabilidad es el lenguaje de la incertidumbre, entonces las **distribuciones de probabilidad** son sus vocablos m√°s importantes. Son modelos matem√°ticos que describen c√≥mo se distribuyen las probabilidades de los posibles resultados de una variable aleatoria. En la ciencia de datos, elegir la distribuci√≥n correcta para modelar un conjunto de datos es un acto de comprensi√≥n profunda del fen√≥meno subyacente. No todas las distribuciones son iguales, y su uso adecuado puede ser la diferencia entre un modelo predictivo exitoso y uno que falla rotundamente. Las distribuciones pueden clasificarse principalmente en discretas y continuas, dependiendo de si la variable aleatoria toma un conjunto contable de valores o un rango continuo.\n",
    "\n",
    "### Distribuci√≥n Normal\n",
    "\n",
    "La distribuci√≥n **normal**, tambi√©n conocida como **gaussiana** o **campana de Gauss**, es quiz√°s la distribuci√≥n m√°s famosa y utilizada. Su importancia radica en gran medida en el **Teorema Central del L√≠mite**, que garantiza que la suma de muchas variables aleatorias independientes e id√©nticamente distribuidas tender√° a distribuirse normalmente. Esto explica por qu√© la distribuci√≥n normal aparece tan com√∫nmente en la naturaleza y en la sociedad, desde las medidas antropom√©tricas como la altura y el peso, pasando por los errores de medici√≥n, hasta los coeficientes de inteligencia (CI). \n",
    "\n",
    "La distribuci√≥n normal est√° completamente definida por dos par√°metros: \n",
    "* su media (Œº), que determina el centro de la campana, y su desviaci√≥n est√°ndar (œÉ), que determina su anchura o dispersi√≥n. Una caracter√≠stica notable es la **regla emp√≠rica o regla 68-95-99.7**: aproximadamente el 68% de los datos caen dentro de una desviaci√≥n est√°ndar de la media, el 95% dentro de dos desviaciones est√°ndar y el 99.7% dentro de tres. Esta regla proporciona una intuici√≥n r√°pida sobre la dispersi√≥n de los datos. \n",
    "* la distribuci√≥n normal est√°ndar, con Œº=0 y œÉ=1, es una versi√≥n universalizada que se usa para calcular probabilidades a trav√©s de los **puntajes z** (que indican cu√°ntas desviaciones est√°ndar est√° un valor de la media)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79858775",
   "metadata": {},
   "source": [
    "### Distribuci√≥n Binomial\n",
    "\n",
    "Otra distribuci√≥n fundamental es la **binomial**. Modela el n√∫mero de √©xitos en un n√∫mero fijo de ensayos independientes (*n*), donde cada ensayo tiene la misma probabilidad de √©xito (*p*). Ejemplos cl√°sicos incluyen el n√∫mero de caras en 10 lanzamientos de una moneda justa o el n√∫mero de pacientes que responden a un tratamiento en un ensayo cl√≠nico. La forma de la distribuci√≥n binomial depende de *p*: es sim√©trica si *p*=0.5 y asim√©trica si *p* es diferente de 0.5. Cuando el n√∫mero de ensayos *n* es grande y la probabilidad de √©xito *p* es peque√±a, la distribuci√≥n binomial puede aproximarse bien por una distribuci√≥n de Poisson."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea040dc5",
   "metadata": {},
   "source": [
    "### Distribuci√≥n de Poisson\n",
    "\n",
    "La distribuci√≥n **de Poisson** modela el n√∫mero de veces que ocurre un evento en un intervalo de tiempo o espacio fijo, dado que estos eventos ocurren con una tasa media constante (*Œª*) y son independientes del tiempo transcurrido desde el √∫ltimo evento. Es ideal para fen√≥menos de conteo, como el n√∫mero de correos electr√≥nicos que recibes por hora, el n√∫mero de accidentes en una intersecci√≥n al d√≠a o el n√∫mero de imperfecciones en un rollo de tela. Un hecho interesante es que tanto la media como la varianza de una distribuci√≥n de Poisson son iguales a Œª. A medida que Œª se hace grande (generalmente > 10 o > 30), la distribuci√≥n de Poisson se vuelve m√°s sim√©trica y puede aproximarse por una distribuci√≥n normal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f99296",
   "metadata": {},
   "source": [
    "### Distribuci√≥n Exponencial\n",
    "\n",
    "Por √∫ltimo, la distribuci√≥n **exponencial** est√° estrechamente relacionada con la distribuci√≥n de Poisson. Mientras que la distribuci√≥n de Poisson modela el n√∫mero de eventos, la distribuci√≥n exponencial modela el tiempo que transcurre entre dichos eventos en un proceso de Poisson. Es decir, si sabemos que los clientes llegan a un negocio a una tasa media de Œª por hora (distribuci√≥n de Poisson), la distribuci√≥n exponencial nos dir√° la probabilidad de cu√°nto tiempo pasar√° antes de que llegue el pr√≥ximo cliente. Esta distribuci√≥n es continua y esencial para modelar tiempos de vida √∫til o tiempos de espera."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cb03c9",
   "metadata": {},
   "source": [
    "La elecci√≥n de la distribuci√≥n correcta es una habilidad cr√≠tica. Por ejemplo, la resoluci√≥n de un sensor de precisi√≥n podr√≠a modelarse con una distribuci√≥n rectangular (donde todos los valores dentro de un rango son igualmente probables). El tiempo de respuesta de un servidor web podr√≠a seguir una distribuci√≥n exponencial. La distribuci√≥n log-normal, que es asim√©trica hacia la derecha, es √∫til para modelar variables que no pueden ser negativas y tienen una larga cola a la derecha, como los salarios o el tama√±o de las c√©lulas. El GUM (Gu√≠a para la Expresi√≥n de la Incertidumbre en el Metrolog√≠a) recomienda estas distribuciones seg√∫n el tipo de conocimiento disponible sobre los l√≠mites y la probabilidad de los valores. Dominar estas distribuciones permite al cient√≠fico de datos construir modelos m√°s realistas y robustos, capaces de capturar la esencia de los datos que est√°n analizando."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b7360b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## üé≤ 1. Introducci√≥n a la Teor√≠a de la Probabilidad\n",
    "\n",
    "La **probabilidad** es una rama de las matem√°ticas que se encarga de cuantificar la **incertidumbre**. Nos permite asignar un valor num√©rico entre 0 (imposible) y 1 (certeza) a la posibilidad de que ocurra un evento.\n",
    "\n",
    "### üîë Conceptos Clave\n",
    "\n",
    "  * **Experimento Aleatorio:** Cualquier proceso cuyo resultado no se puede predecir con certeza (ejemplo: lanzar un dado).\n",
    "  * **Espacio Muestral ($\\Omega$):** El conjunto de todos los posibles resultados de un experimento aleatorio (ejemplo: al lanzar un dado, $\\Omega = \\{1, 2, 3, 4, 5, 6\\}$).\n",
    "  * **Evento (E):** Un subconjunto del espacio muestral, es decir, un resultado o un conjunto de resultados que nos interesa (ejemplo: obtener un n√∫mero par, $E = \\{2, 4, 6\\}$).\n",
    "\n",
    "### üî¢ C√°lculo B√°sico de la Probabilidad\n",
    "\n",
    "La probabilidad simple de que ocurra un evento $E$ se calcula usando la **Regla de Laplace**:\n",
    "\n",
    "$$P(E) = \\frac{\\text{N√∫mero de resultados favorables}}{\\text{N√∫mero total de resultados posibles}}$$\n",
    "\n",
    "### üíª Ejemplo en Python: Probabilidad Simple\n",
    "\n",
    "Podemos simular un experimento simple como lanzar un dado.\n",
    "\n",
    "```python\n",
    "# Importamos la librer√≠a 'random' para simular el lanzamiento\n",
    "import random\n",
    "\n",
    "# Definimos el espacio muestral (resultados posibles)\n",
    "espacio_muestral = [1, 2, 3, 4, 5, 6]\n",
    "\n",
    "# Definimos el evento E: obtener un n√∫mero par\n",
    "evento_e = [2, 4, 6]\n",
    "\n",
    "# C√°lculo de la probabilidad te√≥rica\n",
    "probabilidad_teorica = len(evento_e) / len(espacio_muestral)\n",
    "\n",
    "print(f\"Espacio Muestral (Œ©): {espacio_muestral}\")\n",
    "print(f\"Evento E (n√∫mero par): {evento_e}\")\n",
    "print(f\"P(E) = {len(evento_e)} / {len(espacio_muestral)} = {probabilidad_teorica:.2f}\")\n",
    "```\n",
    "\n",
    "-----\n",
    "\n",
    "Estos fundamentos son la base. El siguiente paso l√≥gico, y crucial para el Teorema de Bayes, es entender la **Probabilidad Condicional**.\n",
    "\n",
    "Pero antes de seguir, a partir de la explicaci√≥n anterior, ¬øc√≥mo definir√≠as la probabilidad de obtener un n√∫mero **menor o igual a 3** al lanzar ese mismo dado? ü§î\n",
    "\n",
    "La probabilidad de obtener un n√∫mero menor o igual a 3 ($\\{1, 2, 3\\}$) era, en efecto, $3/6 = 0.5$. \n",
    "\n",
    "Excelente, has pasado directamente al concepto central: la **Probabilidad Condicional**. Es un paso crucial, ya que el Teorema de Bayes se basa completamente en ella.\n",
    "\n",
    "Antes de seguir, la probabilidad de obtener un n√∫mero menor o igual a 3 ($\\{1, 2, 3\\}$) era, en efecto, $3/6 = 0.5$. ¬°Bien hecho\\!\n",
    "\n",
    "-----\n",
    "\n",
    "## üîó 2. Probabilidad Condicional\n",
    "\n",
    "La **probabilidad condicional** es la probabilidad de que ocurra un **Evento A**, sabiendo que otro **Evento B** ya ha ocurrido. En esencia, estamos **reduciendo nuestro espacio muestral** a la informaci√≥n que ya conocemos.\n",
    "\n",
    "### ‚úçÔ∏è Notaci√≥n y F√≥rmula\n",
    "\n",
    "La probabilidad condicional se denota como $P(A|B)$, que se lee: \"Probabilidad de A, dado B\" o \"Probabilidad de A, condicional a B\".\n",
    "\n",
    "Se calcula con la siguiente f√≥rmula:\n",
    "\n",
    "$$P(A|B) = \\frac{P(A \\cap B)}{P(B)}$$\n",
    "\n",
    "Donde:\n",
    "\n",
    "  * $P(A \\cap B)$ es la probabilidad de que **A y B** ocurran conjuntamente (la intersecci√≥n).\n",
    "  * $P(B)$ es la probabilidad del evento que ya se conoce (la condici√≥n).\n",
    "\n",
    "[Images of Venn diagram conditional probability]\n",
    "\n",
    "### üí° El Impacto del \"Dado Que\"\n",
    "\n",
    "El factor clave es el t√©rmino \"**dado que**\". Nos dice que ya no consideramos todos los posibles resultados ($\\Omega$), sino solo aquellos resultados que cumplen con la condici√≥n $B$.\n",
    "\n",
    "### üíª Ejemplo en Python: Reducci√≥n del Espacio Muestral\n",
    "\n",
    "Volvamos a nuestro dado de 6 caras. Queremos calcular:\n",
    "\n",
    "  * **Evento A:** Obtener un n√∫mero impar ($\\{1, 3, 5\\}$).\n",
    "  * **Evento B:** Obtener un n√∫mero mayor que 3 ($\\{4, 5, 6\\}$).\n",
    "\n",
    "**Pregunta:** ¬øCu√°l es la probabilidad de obtener un n√∫mero impar, *dado que* el n√∫mero fue mayor que 3? $P(A|B)$.\n",
    "\n",
    "```python\n",
    "# Definimos las probabilidades de los eventos\n",
    "P_A = 3/6  # P(impar)\n",
    "P_B = 3/6  # P(mayor que 3)\n",
    "\n",
    "# Calculamos la probabilidad de la intersecci√≥n (A y B)\n",
    "# Los resultados que son impares Y mayores que 3: solo el 5\n",
    "interseccion_A_B = 1/6\n",
    "\n",
    "# Aplicamos la f√≥rmula de probabilidad condicional: P(A|B) = P(A ‚à© B) / P(B)\n",
    "P_A_dado_B = interseccion_A_B / P_B\n",
    "\n",
    "print(f\"P(A ‚à© B) = {interseccion_A_B:.2f}\")\n",
    "print(f\"P(B) = {P_B:.2f}\")\n",
    "print(f\"P(Impar | >3) = P(A|B) = {P_A_dado_B:.3f} (o 1/3)\")\n",
    "```\n",
    "\n",
    "-----\n",
    "\n",
    "Ahora que tenemos esta base, podemos formular una nueva pregunta usando el mismo dado:\n",
    "\n",
    "¬øCu√°l es la probabilidad de obtener un n√∫mero **mayor que 4** (Evento A), **dado que** el resultado fue un **n√∫mero par** (Evento B)?\n",
    "\n",
    "Identificar cu√°les son los resultados de la intersecci√≥n ($A \\cap B$) y la probabilidad de la condici√≥n ($P(B)$).\n",
    "\n",
    "  * **A** (mayor que 4) = $\\{5, 6\\}$\n",
    "  * **B** (par) = $\\{2, 4, 6\\}$\n",
    "  * La intersecci√≥n ($A \\cap B$) es $\\{6\\}$ (un solo resultado).\n",
    "  * $P(A|B) = P(A \\cap B) / P(B) = (1/6) / (3/6) = 1/3$. \n",
    "\n",
    "¬°Buena pregunta\\! Es excelente revisar los fundamentos antes de avanzar, ya que el Teorema de Bayes utiliza estas operaciones constantemente.\n",
    "\n",
    "-----\n",
    "\n",
    "\n",
    "## üéØ 3. Eventos y Operaciones\n",
    "\n",
    "Un **evento** ($\\text{E}$) es, simplemente, un **subconjunto** del espacio muestral ($\\Omega$). Es la descripci√≥n de uno o m√°s resultados que nos interesan de un experimento.\n",
    "\n",
    "Por ejemplo, si el experimento es la extracci√≥n de una carta de una baraja de 52 cartas:\n",
    "\n",
    "  * $\\Omega$ = Las 52 cartas.\n",
    "  * Evento $A$ = Obtener un As.\n",
    "  * Evento $B$ = Obtener una carta de Corazones.\n",
    "\n",
    "### üßÆ Operaciones B√°sicas entre Eventos (Basadas en Teor√≠a de Conjuntos)\n",
    "\n",
    "Las operaciones nos permiten combinar o modificar los eventos para calcular probabilidades m√°s complejas.\n",
    "\n",
    "| Operaci√≥n | S√≠mbolo | Significado | Descripci√≥n de Resultados |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **Uni√≥n** | $\\text{A} \\cup \\text{B}$ | $\\text{A}$ o $\\text{B}$ | Ocurre si $\\text{A}$ ocurre, $\\text{B}$ ocurre, o ambos ocurren. |\n",
    "| **Intersecci√≥n** | $\\text{A} \\cap \\text{B}$ | $\\text{A}$ y $\\text{B}$ | Ocurre solo si $\\text{A}$ y $\\text{B}$ ocurren simult√°neamente. |\n",
    "| **Complemento** | $\\text{A}^\\text{c}$ o $\\text{A}'$ | No $\\text{A}$ | Ocurre si $\\text{A}$ no ocurre. |\n",
    "| **Eventos Mutuamente Excluyentes** | $\\text{A} \\cap \\text{B} = \\emptyset$ | $\\text{A}$ y $\\text{B}$ son Disjuntos | No pueden ocurrir al mismo tiempo (su intersecci√≥n es vac√≠a). |\n",
    "\n",
    "[Images of Venn diagram showing set operations union intersection complement]\n",
    "\n",
    "### üíª Ejemplo de Operaciones en Python (Concepto)\n",
    "\n",
    "Podemos ver las operaciones de manera pr√°ctica usando los conjuntos de Python.\n",
    "\n",
    "**Experimento:** Lanzar un dado ($\\Omega = \\{1, 2, 3, 4, 5, 6\\}$).\n",
    "\n",
    "```python\n",
    "espacio_muestral = {1, 2, 3, 4, 5, 6}\n",
    "A = {1, 2, 3}  # Evento A: Obtener 3 o menos\n",
    "B = {2, 4, 6}  # Evento B: Obtener un n√∫mero par\n",
    "\n",
    "# 1. Uni√≥n (A o B)\n",
    "# Usamos el operador '|' o el m√©todo union()\n",
    "union_A_B = A.union(B)\n",
    "print(f\"Uni√≥n (A U B): {union_A_B}\") # Debe ser {1, 2, 3, 4, 6}\n",
    "\n",
    "# 2. Intersecci√≥n (A y B)\n",
    "# Usamos el operador '&' o el m√©todo intersection()\n",
    "interseccion_A_B = A.intersection(B)\n",
    "print(f\"Intersecci√≥n (A ‚à© B): {interseccion_A_B}\") # Debe ser {2}\n",
    "```\n",
    "\n",
    "-----\n",
    "\n",
    "Ahora es tu turno de practicar una operaci√≥n fundamental. Usando los mismos conjuntos:\n",
    "\n",
    "**Evento A** = $\\{1, 2, 3\\}$\n",
    "\n",
    "¬øCu√°l es el **Complemento de A** ($A^c$)? ¬øQu√© resultados incluye y cu√°l es su probabilidad $P(A^c)$? ü§î\n",
    "\n",
    "Para el evento **A** (Obtener 3 o menos) = $\\{1, 2, 3\\}$, el **Complemento de A** ($A^c$) es $\\{4, 5, 6\\}$, que son los resultados de \"obtener m√°s de 3\". Su probabilidad es $P(A^c) = 3/6 = 0.5$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6264a8",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## üìä 4. Probabilidad Marginal\n",
    "\n",
    "La **probabilidad marginal** de un evento es la probabilidad de que ese evento ocurra, calculada **sin tener en cuenta ninguna otra variable** (o \"marginando\" la informaci√≥n sobre otras variables).\n",
    "\n",
    "Imagina que tienes una tabla de datos (una tabla de contingencia) que muestra la cantidad de hombres y mujeres que votaron 'S√≠' o 'No'. La probabilidad marginal ser√≠a la probabilidad de \"votar S√≠\" (sin importar si fue hombre o mujer) o la probabilidad de \"ser mujer\" (sin importar si vot√≥ S√≠ o No).\n",
    "\n",
    "### üÜö Contraste con Otros Tipos\n",
    "\n",
    "* **Probabilidad Marginal ($P(A)$):** La probabilidad de un solo evento. *Ejemplo: ¬øCu√°l es la probabilidad de que llueva?*\n",
    "* **Probabilidad Conjunta ($P(A \\cap B)$):** La probabilidad de que dos eventos ocurran juntos. *Ejemplo: ¬øCu√°l es la probabilidad de que llueva **y** sea fin de semana?*\n",
    "* **Probabilidad Condicional ($P(A|B)$):** La probabilidad de A dado que B ya ocurri√≥. *Ejemplo: ¬øCu√°l es la probabilidad de que llueva, **dado que** hay nubes negras?*\n",
    "\n",
    "### ‚ûï La Ley de la Probabilidad Total (C√°lculo Marginal)\n",
    "\n",
    "En el contexto del Teorema de Bayes, a menudo no conocemos la probabilidad marginal $P(A)$ directamente. En cambio, usamos la **Ley de la Probabilidad Total (LPT)**, que nos permite calcular $P(A)$ sumando las probabilidades conjuntas de $A$ con todos los posibles escenarios ($B_i$) que pueden causar $A$.\n",
    "\n",
    "$$P(A) = \\sum_{i} P(A \\mid B_i) P(B_i)$$\n",
    "\n",
    "Donde $B_i$ son eventos mutuamente excluyentes y exhaustivos (cubren todo el espacio muestral).\n",
    "\n",
    "En el mundo real, esta ley nos permite encontrar la probabilidad general de un resultado (por ejemplo, \"que una persona d√© positivo en un test m√©dico\") considerando todas las formas en que ese resultado puede ocurrir (dado que est√° enferma **O** dado que no est√° enferma).\n",
    "\n",
    "---\n",
    "\n",
    "Ahora que hemos cubierto la Probabilidad Condicional y la Marginal, estamos listos para unir estos conceptos en el teorema m√°s poderoso de la probabilidad.\n",
    "\n",
    "El **Teorema de Bayes** es esencialmente una herramienta para **actualizar nuestras creencias** basadas en nueva evidencia.\n",
    "\n",
    "$$\\text{P(Hip√≥tesis | Evidencia)} = \\frac{\\text{P(Evidencia | Hip√≥tesis)} \\cdot \\text{P(Hip√≥tesis)}}{\\text{P(Evidencia)}}$$\n",
    "\n",
    "En esta f√≥rmula, el t√©rmino $P(\\text{Evidencia})$ en el denominador es precisamente una **Probabilidad Marginal** (calculada con la Ley de la Probabilidad Total).\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
