{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1198b05a",
   "metadata": {},
   "source": [
    "\n",
    "#  El Teorema de Bayes\n",
    "\n",
    "El **Teorema de Bayes** es un principio fundamental en la probabilidad que nos permite **actualizar la probabilidad** de una hip贸tesis ($H$) a la luz de una nueva evidencia o informaci贸n ($E$). Es decir, nos ayuda a pasar de una creencia inicial a una creencia posterior m谩s informada.\n",
    "\n",
    "Es fundamental entender que el Teorema de Bayes no es solo una f贸rmula, sino una **metodolog铆a para actualizar el conocimiento** en cualquier campo. La clave en todos estos ejemplos es siempre **actualizar la creencia previa** con la nueva evidencia.\n",
    "\n",
    "##  Definici贸n y la F贸rmula de Bayes\n",
    "\n",
    "La f贸rmula establece c贸mo la probabilidad de un evento $H$ (la hip贸tesis) cambia cuando se observa un evento $E$ (la evidencia).\n",
    "\n",
    "$$P(H|E) = \\frac{P(E|H) \\cdot P(H)}{P(E)}$$\n",
    "\n",
    "[Images of Bayesian inference]\n",
    "\n",
    "| T茅rmino | Nombre | Significado Conceptual |\n",
    "| :--- | :--- | :--- |\n",
    "|  $P(H|E)$ | **Probabilidad Posterior** | Lo que queremos saber: la probabilidad de la hip贸tesis *despu茅s* de ver la evidencia. |\n",
    "|  $P(E|H)$ | **Verosimilitud (Likelihood)** | La probabilidad de ver la evidencia, *dada* que la hip贸tesis es verdadera. |\n",
    "|  $P(H)$ | **Probabilidad Previa (Prior)** | Nuestra creencia inicial sobre la hip贸tesis *antes* de ver la evidencia. |\n",
    "|  $P(E)$ | **Evidencia/Probabilidad Marginal** | La probabilidad de la evidencia en general. Se calcula usando la Ley de Probabilidad Total. |\n",
    "\n",
    "##  Ejemplos Cotidianos de su Uso\n",
    "\n",
    "El teorema de Bayes se utiliza constantemente en sistemas donde las predicciones deben adaptarse a nuevos datos.\n",
    "\n",
    "  * **1.  Filtros de Spam (Tecnolog铆a):**\n",
    "\n",
    "      * Los filtros de correo usan Bayes para calcular $P(\\text{Spam} | \\text{Palabra X})$. El sistema aprende que si un correo contiene la palabra **$E$** (\"farmacia\"), la probabilidad de que sea spam **$H$** aumenta significativamente respecto a la probabilidad previa ($P(H)$) de cualquier correo aleatorio de la bandeja de entrada.\n",
    "\n",
    "  * **2. ┖ Diagn贸stico M茅dico (Salud):**\n",
    "\n",
    "      * Como vimos, se usa para calcular $P(\\text{Enfermo} | \\text{Positivo})$. El resultado depende tanto de la **precisi贸n del test** ($P(E|H)$) como de la **prevalencia de la enfermedad** en la poblaci贸n ($P(H)$). Si una enfermedad es muy rara, es m谩s probable que un positivo sea un falso positivo, incluso con un test muy bueno.\n",
    "\n",
    "  * **3.  Finanzas y Mercados:**\n",
    "\n",
    "      * Los analistas usan modelos bayesianos para actualizar las probabilidades de un evento econ贸mico (ej. una recesi贸n) a medida que se publican nuevos datos (ej. tasas de desempleo).\n",
    "\n",
    "##  Ejemplo Pr谩ctico en Python: El Test M茅dico\n",
    "\n",
    "Retomemos el ejemplo m茅dico para ver la fuerza de la **Probabilidad Previa ($P(H)$)**.\n",
    "\n",
    "**Escenario:**\n",
    "\n",
    "  * **Enfermedad Rara ($P(H)$):** 0.1% (0.001)\n",
    "  * **Test Preciso ($P(E|H)$):** 99% (0.99)\n",
    "  * **Falsa Alarma ($P(E|\\text{No } H)$):** 5% (0.05)\n",
    "\n",
    "**Objetivo:** Calcular $P(\\text{Enfermo} | \\text{Positivo})$.\n",
    "\n",
    "```python\n",
    "# 1. Definici贸n de las Probabilidades (Datos Iniciales)\n",
    "P_H = 0.001       # P(Enfermo): Probabilidad Previa (Prior)\n",
    "P_no_H = 1 - P_H  # P(No Enfermo) = 0.999\n",
    "\n",
    "P_E_dado_H = 0.99 # P(Positivo | Enfermo): Verosimilitud (Sensibilidad)\n",
    "P_E_dado_no_H = 0.05 # P(Positivo | No Enfermo): Tasa de Falso Positivo\n",
    "\n",
    "# 2. C谩lculo de la Probabilidad Marginal P(E) (Denominador - Ley de Probabilidad Total)\n",
    "# P(E) = P(Positivo|Enfermo) * P(Enfermo) + P(Positivo|No Enfermo) * P(No Enfermo)\n",
    "P_E = (P_E_dado_H * P_H) + (P_E_dado_no_H * P_no_H)\n",
    "\n",
    "# 3. Aplicaci贸n del Teorema de Bayes\n",
    "# P(H|E) = [P(E|H) * P(H)] / P(E)\n",
    "P_H_dado_E = (P_E_dado_H * P_H) / P_E\n",
    "\n",
    "print(f\"Probabilidad de la Evidencia P(E): {P_E:.4f}\")\n",
    "print(f\"Probabilidad Posterior P(Enfermo | Positivo): {P_H_dado_E:.3f}\")\n",
    "```\n",
    "\n",
    "-----\n",
    "\n",
    "El resultado final, $P(\\text{Enfermo} | \\text{Positivo}) \\approx 0.019$ (o 1.9%), es muy bajo.\n",
    "\n",
    "驴Por qu茅 la probabilidad de estar realmente enfermo es solo del **1.9%** a pesar de que el test tiene una precisi贸n del 99%? 驴Qu茅 factor de los datos iniciales es el m谩s determinante en este sorprendente resultado? \n",
    "\n",
    "El resultado sorprendentemente bajo ($1.9\\%$) para $P(\\text{Enfermo} | \\text{Positivo})$ se debe a una de las probabilidades iniciales. \n",
    "\n",
    "Esa es una observaci贸n clave. La **Probabilidad Previa** $P(H)$ es, de lejos, el factor m谩s determinante en este resultado. Aqu铆 te muestro por qu茅, en t茅rminos de la f贸rmula de Bayes: \n",
    "\n",
    "$$P(H|E) = \\frac{\\overbrace{P(E|H) \\cdot P(H)}^{\\text{Casos Verdaderos Positivos}}}{\\underbrace{P(E|H) \\cdot P(H) + P(E|\\text{No } H) \\cdot P(\\text{No } H)}_{\\text{Casos Positivos Totales (Evidencia, } P(E))}}$$\n",
    "\n",
    "###  El Impacto de $P(H)$\n",
    "\n",
    "1.  **El Numerador (Casos Verdaderos Positivos):** La probabilidad de que un resultado positivo sea verdadero es directamente proporcional a $P(H)$. Dado que $P(H)$ es muy peque帽a ($0.001$), el numerador es diminuto: $0.99 \\times 0.001 = 0.00099$.\n",
    "\n",
    "2.  **El Denominador (Casos Positivos Totales):** La evidencia total $P(E)$ est谩 compuesta por dos v铆as para obtener un positivo:\n",
    "    * **Verdaderos Positivos** ($0.00099$).\n",
    "    * **Falsos Positivos** ($P(E|\\text{No } H) \\cdot P(\\text{No } H)$). Este t茅rmino es $0.05 \\times 0.999 \\approx 0.04995$.\n",
    "\n",
    "El valor de la evidencia ($P(E) \\approx 0.05$) est谩 **dominado por los falsos positivos** ($0.04995$) porque la poblaci贸n sana ($P(\\text{No } H)$) es 999 veces m谩s grande que la poblaci贸n enferma.\n",
    "\n",
    "Al dividir el numerador muy peque帽o ($0.00099$) entre un denominador mucho m谩s grande que 茅l ($0.05$), el resultado final ($P(H|E) \\approx 1.9\\%$) permanece bajo. **La rareza de la enfermedad ($P(H)$) supera la precisi贸n del test ($P(E|H)$).**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dce36b8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##  Aplicaciones de Bayes en Diversas Industrias\n",
    "\n",
    "### 锔 1. Geolog铆a y Exploraci贸n de Recursos\n",
    "\n",
    "En la b煤squeda de petr贸leo, gas o dep贸sitos minerales, las empresas enfrentan una gran incertidumbre.\n",
    "\n",
    "* **Hip贸tesis ($H$):** Existe un dep贸sito comercialmente viable en una ubicaci贸n espec铆fica.\n",
    "* **Evidencia ($E$):** Los datos s铆smicos arrojaron un patr贸n positivo (indicando posible estructura).\n",
    "\n",
    "Antes de la perforaci贸n, la **Probabilidad Previa ($P(H)$)** puede ser muy baja (ej. 10%). Sin embargo, si los costosos y complejos datos s铆smicos ($E$) son positivos, se usa Bayes para calcular la **Probabilidad Posterior ($P(H|E)$)**. Esta probabilidad actualizada es la que se utiliza para tomar la decisi贸n de invertir millones en la perforaci贸n.\n",
    "\n",
    "###  2. Rob贸tica y Navegaci贸n Aut贸noma\n",
    "\n",
    "Los robots, drones y veh铆culos aut贸nomos usan t茅cnicas bayesianas, como el **Filtro de Kalman**, para estimar su ubicaci贸n en tiempo real.\n",
    "\n",
    "* **Hip贸tesis ($H$):** La ubicaci贸n actual del robot en el mapa.\n",
    "* **Evidencia ($E$):** Las nuevas lecturas de los sensores (ej. GPS, LiDAR).\n",
    "\n",
    "Cada nueva lectura de un sensor ($E$) se toma como evidencia que **actualiza** la creencia previa sobre la ubicaci贸n. Si el GPS da una ubicaci贸n, esta es la *evidencia*. El sistema usa Bayes para ponderar esa evidencia con su 煤ltima estimaci贸n conocida (la *Prior*) y generar una nueva y m谩s precisa estimaci贸n (la *Posterior*). Esto permite que la navegaci贸n sea fluida y precisa, a pesar del ruido o error en los sensores.\n",
    "\n",
    "\n",
    "![](https://encrypted-tbn2.gstatic.com/licensed-image?q=tbn:ANd9GcSby6f7hmf4dsSDRE6kSnI9HvtDunMnM4lF0xgrcVXTaGvVR4rL63YNFuZ7x0ilm7wurhzAMIREZYZKG2ELe1m5MiNqGwZBWeaLUg_YRDom2CuukoY)\n",
    "\n",
    "\n",
    "### 锔 3. Ciencia Forense y Legal\n",
    "\n",
    "En los tribunales, Bayes ayuda a cuantificar el valor de la evidencia, especialmente la evidencia estad铆stica como el ADN.\n",
    "\n",
    "* **Hip贸tesis ($H$):** El acusado es el responsable.\n",
    "* **Evidencia ($E$):** Se encontr贸 una coincidencia de ADN entre el acusado y la escena del crimen.\n",
    "\n",
    "La f贸rmula de Bayes ayuda a los expertos a determinar la probabilidad de que la coincidencia de ADN ocurra **dado que** el acusado es inocente ($P(E|\\text{No } H)$), contra la probabilidad de que la coincidencia ocurra **dado que** el acusado es culpable ($P(E|H)$). Esto permite a los jurados concentrarse en el verdadero valor probatorio de la evidencia.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b435d399",
   "metadata": {},
   "source": [
    "El **Filtro de Kalman** es un tema fascinante y la aplicaci贸n m谩s com煤n del pensamiento bayesiano en ingenier铆a.\n",
    "\n",
    "##  Filtro de Kalman\n",
    "\n",
    "El Filtro de Kalman es un **algoritmo recursivo** que utiliza una serie de mediciones observadas a lo largo del tiempo (a menudo ruidosas o con errores) para producir una **estimaci贸n 贸ptima** de un estado desconocido. Es esencialmente un \"promediador inteligente\" que da m谩s peso a las estimaciones m谩s precisas.\n",
    "\n",
    "Se utiliza masivamente en:\n",
    "* **Navegaci贸n:** Para combinar lecturas de GPS, aceler贸metros y giroscopios en veh铆culos aut贸nomos o drones.\n",
    "* **Econom铆a:** Para estimar tendencias ocultas.\n",
    "* **Rob贸tica:** Para localizar robots en un mapa (localizaci贸n simult谩nea y mapeo o SLAM).\n",
    "\n",
    "###  El Ciclo Bayesiano en Acci贸n\n",
    "\n",
    "El poder del Filtro de Kalman radica en su naturaleza **recursiva**, que sigue el mismo patr贸n de la inferencia bayesiana que acabamos de estudiar:\n",
    "\n",
    "1.  **Fase de Predicci贸n (Prior):** El filtro toma la estimaci贸n del estado actual (la **Probabilidad Posterior** del paso anterior) y la usa como **Probabilidad Previa** para predecir el estado futuro. Esta predicci贸n se basa en las leyes de movimiento del sistema (f铆sica, en rob贸tica).\n",
    "2.  **Fase de Actualizaci贸n (Posterior):** Cuando llega una nueva medici贸n del sensor (la **Evidencia**), el filtro calcula el error entre su predicci贸n y la medici贸n real. Luego, utiliza esta informaci贸n, junto con la f贸rmula de Bayes, para **actualizar** la estimaci贸n, creando la nueva **Probabilidad Posterior** (el mejor estimado de la realidad).\n",
    "\n",
    "\n",
    "\n",
    "De esta forma, el filtro no necesita recordar toda la historia de mediciones; solo necesita la estimaci贸n anterior para calcular la siguiente, lo que lo hace muy eficiente computacionalmente.\n",
    "\n",
    "Si volvemos a la f贸rmula de Bayes:\n",
    "\n",
    "$$P(H|E) = \\frac{P(E|H) \\cdot P(H)}{P(E)}$$\n",
    "\n",
    "驴Qu茅 t茅rmino de la f贸rmula de Bayes crees que corresponde a la estimaci贸n que produce la **Fase de Predicci贸n** del Filtro de Kalman, antes de que llegue la nueva medici贸n ($E$)? \n",
    "\n",
    "Esa es una excelente pregunta que conecta la **Teor铆a de Bayes** directamente con su aplicaci贸n pr谩ctica en ingenier铆a. Est谩s en lo correcto al enfocarte en la fase de **Predicci贸n** del Filtro de Kalman. \n",
    "\n",
    "###  El Filtro de Kalman y la Inferencia Bayesiana\n",
    "\n",
    "Recordemos los dos componentes esenciales de la f贸rmula de Bayes:\n",
    "\n",
    "$$P(H|E) = \\frac{P(E|H) \\cdot P(H)}{P(E)}$$\n",
    "\n",
    "| Etapa de Inferencia | T茅rmino Bayesiano | Significado en el Filtro de Kalman |\n",
    "| :--- | :--- | :--- |\n",
    "| **Antes de la Evidencia** | $P(H)$ **(Probabilidad Previa o Prior)** | Es la creencia o estimaci贸n del estado actual *antes* de recibir la nueva lectura del sensor. |\n",
    "| **Despu茅s de la Evidencia** | $P(H|E)$ **(Probabilidad Posterior o Posterior)** | Es la estimaci贸n *actualizada* y m谩s precisa del estado, despu茅s de incorporar la nueva lectura del sensor. |\n",
    "\n",
    "El t茅rmino de la f贸rmula de Bayes que representa la predicci贸n que el filtro genera *antes* de que llegue la nueva evidencia ($E$) es la **Probabilidad Previa** $P(H)$.\n",
    "\n",
    "###  驴Por qu茅 $P(H)$ es la Predicci贸n?\n",
    "\n",
    "* La **Predicci贸n** se basa 煤nicamente en la estimaci贸n anterior y en las leyes de movimiento del sistema. No ha visto la nueva medici贸n del sensor.\n",
    "* En la f贸rmula de Bayes, $P(H)$ es la probabilidad de la hip贸tesis **antes** de considerar la nueva evidencia $E$.\n",
    "\n",
    "Por lo tanto, el ciclo del Filtro de Kalman se puede mapear directamente a los componentes de la inferencia bayesiana:\n",
    "\n",
    "|  Ciclo del Filtro de Kalman | T茅rmino de Bayes | Nombre en Bayes |\n",
    "| :--- | :--- | :--- |\n",
    "| **Predicci贸n** | $P(H)$ | **Probabilidad Previa** (Prior) |\n",
    "| **Actualizaci贸n** | $P(H|E)$ | **Probabilidad Posterior** (Posterior) |\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
